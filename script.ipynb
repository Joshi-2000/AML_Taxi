{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Hyperparameters and Comparison between Q-Learning and SARSA Algorithm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib qt5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learn(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate):\n",
    "    '''\n",
    "    Training a q-table with the Q-Learning algorithm\n",
    "    '''\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    # Initialize the q-table with zeros\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # Random generator\n",
    "    rng =np.random.default_rng()\n",
    "\n",
    "    # Start training for num_episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and initialize counters\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        rewards_current_episode  = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Choose an action in the current state\n",
    "            action = eps_greedy(env, q_table, state, exploration_rate)\n",
    "\n",
    "            # Take the action and get the new state and reward\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Update with Bellman Equation Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            q_table[state, action] = q_table[state, action] +  learning_rate * (reward + discount_rate * np.max(q_table[new_state, :] - q_table[state, action]))\n",
    "    \n",
    "            # Count rewards and steps\n",
    "            rewards_current_episode  += reward\n",
    "            step += 1\n",
    "            \n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "            \n",
    "            # If done: finish episode\n",
    "            if done or truncated: \n",
    "                break\n",
    "            \n",
    "        # Reduce exploration\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(step)\n",
    "    return q_table, rewards_all_episodes, steps_all_episodes\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate):\n",
    "    '''\n",
    "    Training a q-table with the SARSA algorithm\n",
    "    '''\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    # Initialize the q-table with zeros\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # Random generator\n",
    "    rng =np.random.default_rng()\n",
    "\n",
    "    # Start training for num_episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and initialize counters\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        rewards_current_episode  = 0\n",
    "        # Choose an action in the current state\n",
    "        action = eps_greedy(env, q_table, state, exploration_rate)\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            \n",
    "            # Take the action and get the new state and reward\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Choose an action in for the new state\n",
    "            new_action = eps_greedy(env, q_table, new_state, exploration_rate)\n",
    "\n",
    "            # Update using Bellman Equation Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * Q(s',a') - Q(s,a)]\n",
    "            q_table[state, action] = q_table[state, action]  +  learning_rate * (reward + discount_rate * q_table[new_state, new_action] - q_table[state, action])\n",
    "    \n",
    "            # Count rewards and steps\n",
    "            rewards_current_episode  += reward\n",
    "            step += 1\n",
    "            \n",
    "            # Our new state is state\n",
    "            # Our new action is action\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "            \n",
    "            # If done: finish episode\n",
    "            if done or truncated: \n",
    "                break\n",
    "            \n",
    "        # Reduce exploration\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(step)\n",
    "\n",
    "        \n",
    "    return q_table, rewards_all_episodes, steps_all_episodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy and non Greedy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(q_table, state):\n",
    "    '''\n",
    "    Greedy Policy\n",
    "\n",
    "    Returns the index of the highest action value for current state\n",
    "    '''\n",
    "    return np.argmax(q_table[state,:])\n",
    "\n",
    "def eps_greedy(env, q_table, state, exploration_rate):\n",
    "    '''\n",
    "    Epsilon-Greedy Policy\n",
    "\n",
    "    Generates a random number between 0 and 1 and checks if the current exploration rate (epsilon) is lower.\n",
    "    In case it is lower it returns the index of the highest action value for current state.\n",
    "    Otherwise it chooses exploring and takes a random action\n",
    "    '''\n",
    "    exploration_rate_threshold = random.uniform(0, 1)\n",
    "    if exploration_rate_threshold > exploration_rate:\n",
    "        return greedy(q_table, state)\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms for Rewards and Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_training(rewards_all_episodes, steps_all_episodes, num_episodes):\n",
    "    '''\n",
    "    Evaluates the training by calculating the means and plotting histograms of Rewards and Steps needed during training for 10 different subsets\n",
    "    '''\n",
    "\n",
    "    # Splits both arrays into multiple subarrays\n",
    "    rewards_per_x_episodes = np.split(np.array(rewards_all_episodes),num_episodes/(num_episodes/10))\n",
    "    steps_per_x_episodes = np.split(np.array(steps_all_episodes),num_episodes/(num_episodes/10))\n",
    "    count = num_episodes/10\n",
    "\n",
    "    # Prints average rewards and steps for ten sub intervalls \n",
    "    print(\"********Average reward and steps per \",num_episodes/10 ,\" episodes********\\n\")\n",
    "    for r, s in zip(rewards_per_x_episodes, steps_per_x_episodes):\n",
    "        print(count, \": Reward = \", str(round(sum(r/(num_episodes/10)), 2)), \", Steps = \", str(round(sum(s/(num_episodes/10)), 2)))\n",
    "        count += num_episodes/10\n",
    "\n",
    "    # initialize Reward Histgram Plot\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "    fig.suptitle('Reward Histograms per ' + str(int(num_episodes/10)) + ' episodes during training')\n",
    "\n",
    "    # Plot rewards\n",
    "    for i in range(len(rewards_per_x_episodes)):\n",
    "        count += num_episodes/10\n",
    "        sns.histplot(ax=axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5], x = rewards_per_x_episodes[i])\n",
    "        axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5].set_title( str(len(rewards_per_x_episodes[i])*(i+1)-int(num_episodes/10)) + \" - \" + str(len(rewards_per_x_episodes[i])*(i+1)))\n",
    "\n",
    "    # initialize Step Histgram Plot\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "    fig.suptitle('Step Histograms per ' + str(int(num_episodes/10)) + ' episodes during training')\n",
    "\n",
    "    # Plot Steps\n",
    "    for i in range(len(steps_per_x_episodes)):\n",
    "        count += num_episodes/10\n",
    "        sns.histplot(ax=axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5], x = steps_per_x_episodes[i])\n",
    "        axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5].set_title( str(len(rewards_per_x_episodes[i])*(i+1)-int(num_episodes/10)) + \" - \" + str(len(rewards_per_x_episodes[i])*(i+1)))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineplots for Rewards and Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithm(x,y,plt_type):\n",
    "    '''\n",
    "    Function which compares the different algorithms \n",
    "    '''\n",
    "    x_series = pd.Series(x)\n",
    "    x_series = x_series.rolling(15).mean()\n",
    "    plt.plot(x_series)\n",
    "\n",
    "    y_series = pd.Series(y)\n",
    "    y_series = y_series.rolling(15).mean()\n",
    "    plt.plot(y_series)\n",
    "\n",
    "    plt.legend([\"Q-Learning\", \"SARSA\"])\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(plt_type)\n",
    "\n",
    "    # plt.yscale(\"symlog\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineplots for Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_params(x,y):\n",
    "    '''\n",
    "    Function which compares the same algorithm with different parameters\n",
    "    '''\n",
    "    step_series = pd.Series(x)\n",
    "    step_series = step_series.rolling(15).mean()\n",
    "    plt.plot(step_series)\n",
    "\n",
    "    step_series = pd.Series(y)\n",
    "    step_series = step_series.rolling(15).mean()\n",
    "    plt.plot(step_series)\n",
    "\n",
    "    #plt.title(\"Sports Watch Data\")\n",
    "    #plt.legend([\"discount_rate = 0.99\", \"discount_rate = 0.2\", \"learning_rate  = 0.8\"])\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "\n",
    "    #plt.yscale(\"symlog\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation on what the agent does after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(env, q_table, max_steps_per_episode, num_vis_episodes=3):\n",
    "    '''\n",
    "    Function which simulates and displays the game a certain amount of times.\n",
    "    '''\n",
    "    # Loop over episodes to visualise\n",
    "    for episode in range(num_vis_episodes):\n",
    "        state, _ = env.reset()\n",
    "        print(\"********EPISODE \", episode+1, \"********\\n\\n\\n\\n\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Render environment for each step and sleep 0.3 seconds\n",
    "        for step in range(max_steps_per_episode):\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            action = np.argmax(q_table[state,:])\n",
    "            new_state, reward, done, truncated, _ = env.step(action)\n",
    "            if done or truncated: \n",
    "                clear_output(wait=True)\n",
    "                print(env.render())\n",
    "                if reward > 0:\n",
    "                    print(\"****You reached the goal!****\")\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"****Something bad happened!****\")\n",
    "                    time.sleep(3)\n",
    "                    clear_output(wait=True)\n",
    "                break\n",
    "            state = new_state\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics for evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(env, num_test_episodes, max_steps_per_episode, q_table):\n",
    "    '''\n",
    "    Function which tests the trained q-table without using exploration\n",
    "    '''\n",
    "    # initialize counters\n",
    "    total_steps, total_penalties, total_reward, total_errors = 0, 0, 0, 0\n",
    "\n",
    "    # Loop over episodes to analyze\n",
    "    for episode in range(num_test_episodes):\n",
    "            state, _ = env.reset()\n",
    "\n",
    "            # Play the game until the agent dies or succeeds and calculate metrics\n",
    "            for step in range(max_steps_per_episode):\n",
    "                total_steps += 1\n",
    "                action = np.argmax(q_table[state,:])\n",
    "                new_state, reward, done, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if reward == -10:\n",
    "                    total_penalties += 1\n",
    "\n",
    "                if done or truncated: \n",
    "                    if reward > 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        total_errors += 1\n",
    "                        break\n",
    "                \n",
    "                if step == max_steps_per_episode  - 1:\n",
    "                    total_errors += 1\n",
    "\n",
    "                state = new_state\n",
    "    \n",
    "    # Print Metrics\n",
    "    print(\"Avg Steps : \", str(round(total_steps/num_test_episodes, 2)))\n",
    "    print(\"Total Penalties: \", str(int(total_penalties)))\n",
    "    print(\"Avg Reward : \", str(round(total_reward/num_test_episodes, 2)))\n",
    "    print(\"Total Errors: \", str(int(total_errors)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Environment and Setup Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Selection and Hyperparameters\n",
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "# env = gym.make(\"CliffWalking-v0\", render_mode=\"ansi\")\n",
    "# env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode='ansi').env \n",
    "\n",
    "params1 = [\n",
    "    0.1,    #learning rate\n",
    "    0.99,   #discount rate\n",
    "    3000,   #num of Episodes\n",
    "    1000,   #max steps per Episode\n",
    "    1,      #exploration rate\n",
    "    1,      #max explaration rate\n",
    "    0.05,   #min exploration rate\n",
    "    0.005,  #exploration decay rate\n",
    "]\n",
    "\n",
    "params2 = [\n",
    "    0.1,    #learning rate\n",
    "    0.99,   #discount rate\n",
    "    1300,   #num of Episodes\n",
    "    1000,   #max steps per Episode\n",
    "    1,      #exploration rate\n",
    "    1,      #max explaration rate\n",
    "    0.01,   #min exploration rate\n",
    "    0.005,  #exploration decay rate\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train different q-Tables with different Parameters and Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table1, reward_list1, step_list1 = train_q_learn(env, *params1)\n",
    "s_q_table1, s_reward_list1, s_step_list1 = train_sarsa(env, *params1)\n",
    "q_table2, reward_list2, step_list2 = train_q_learn(env, *params2)\n",
    "s_q_table2, s_reward_list2, s_step_list2 = train_sarsa(env, *params2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_algorithm(reward_list1, s_reward_list1,  \"Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_algorithm(step_list1, s_step_list1,  \"Steps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_params(reward_list1, reward_list2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Training with metrics + histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward and steps per  300.0  episodes********\n",
      "\n",
      "300.0 : Reward =  -859.69 , Steps =  292.95\n",
      "600.0 : Reward =  -58.04 , Steps =  53.21\n",
      "900.0 : Reward =  -10.13 , Steps =  25.58\n",
      "1200.0 : Reward =  1.98 , Steps =  16.71\n",
      "1500.0 : Reward =  3.85 , Steps =  15.23\n",
      "1800.0 : Reward =  4.44 , Steps =  14.46\n",
      "2100.0 : Reward =  4.73 , Steps =  14.23\n",
      "2400.0 : Reward =  5.23 , Steps =  13.73\n",
      "2700.0 : Reward =  5.69 , Steps =  13.9\n",
      "3000.0 : Reward =  5.21 , Steps =  13.69\n",
      "********Average reward and steps per  300.0  episodes********\n",
      "\n",
      "300.0 : Reward =  -939.89 , Steps =  322.76\n",
      "600.0 : Reward =  -66.74 , Steps =  58.58\n",
      "900.0 : Reward =  -21.84 , Steps =  34.83\n",
      "1200.0 : Reward =  -3.47 , Steps =  21.41\n",
      "1500.0 : Reward =  2.73 , Steps =  16.08\n",
      "1800.0 : Reward =  4.01 , Steps =  15.01\n",
      "2100.0 : Reward =  4.31 , Steps =  14.68\n",
      "2400.0 : Reward =  4.46 , Steps =  14.47\n",
      "2700.0 : Reward =  5.44 , Steps =  14.0\n",
      "3000.0 : Reward =  5.3 , Steps =  14.02\n"
     ]
    }
   ],
   "source": [
    "eval_training(reward_list1, step_list1, params1[2])\n",
    "eval_training(s_reward_list1, s_step_list1, params1[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise environment and agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "visualise(env, s_q_table1, params1[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics for test scenario without exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Steps :  13.45\n",
      "Total Penalties:  0\n",
      "Avg Reward :  7.55\n",
      "Total Errors:  0\n"
     ]
    }
   ],
   "source": [
    "eval_test(env, num_test_episodes=100, max_steps_per_episode=params1[3], q_table=q_table1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa7e97b85bae90f199f0c94d9acbda0c8fe2596009df7ae1a0455e95b822e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
