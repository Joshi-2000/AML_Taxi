{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Hyperparameters and Comparison between Q-Learning and SARSA Algorithm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib qt5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learn(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate):\n",
    "    '''\n",
    "    Training a q-table with the Q-Learning algorithm\n",
    "    '''\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    # Initialize the q-table with zeros\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # Random generator\n",
    "    rng =np.random.default_rng()\n",
    "\n",
    "    # Start training for num_episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and initialize counters\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        rewards_current_episode  = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Choose an action in the current state\n",
    "            action = eps_greedy(env, q_table, state, exploration_rate)\n",
    "\n",
    "            # Take the action and get the new state and reward\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Update with Bellman Equation Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            q_table[state, action] = q_table[state, action] +  learning_rate * (reward + discount_rate * np.max(q_table[new_state, :] - q_table[state, action]))\n",
    "    \n",
    "            # Count rewards and steps\n",
    "            rewards_current_episode  += reward\n",
    "            step += 1\n",
    "            \n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "            \n",
    "            # If done: finish episode\n",
    "            if done or truncated: \n",
    "                break\n",
    "            \n",
    "        # Reduce exploration\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(step)\n",
    "    return q_table, rewards_all_episodes, steps_all_episodes\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate):\n",
    "    '''\n",
    "    Training a q-table with the SARSA algorithm\n",
    "    '''\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    # Initialize the q-table with zeros\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    print(q_table.shape)\n",
    "\n",
    "    # Random generator\n",
    "    rng =np.random.default_rng()\n",
    "\n",
    "    # Start training for num_episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and initialize counters\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        rewards_current_episode  = 0\n",
    "        # Choose an action in the current state\n",
    "        action = eps_greedy(env, q_table, state, exploration_rate)\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            \n",
    "            # Take the action and get the new state and reward\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Choose an action in for the new state\n",
    "            new_action = eps_greedy(env, q_table, new_state, exploration_rate)\n",
    "\n",
    "            # Update using Bellman Equation Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * Q(s',a') - Q(s,a)]\n",
    "            q_table[state, action] = q_table[state, action]  +  learning_rate * (reward + discount_rate * q_table[new_state, new_action] - q_table[state, action])\n",
    "    \n",
    "            # Count rewards and steps\n",
    "            rewards_current_episode  += reward\n",
    "            step += 1\n",
    "            \n",
    "            # Our new state is state\n",
    "            # Our new action is action\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "            \n",
    "            # If done: finish episode\n",
    "            if done or truncated: \n",
    "                break\n",
    "            \n",
    "        # Reduce exploration\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(step)\n",
    "\n",
    "        \n",
    "    return q_table, rewards_all_episodes, steps_all_episodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy and non Greedy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(q_table, state):\n",
    "    '''\n",
    "    Greedy Policy\n",
    "\n",
    "    Returns the index of the highest action value for current state\n",
    "    '''\n",
    "    return np.argmax(q_table[state,:])\n",
    "\n",
    "def eps_greedy(env, q_table, state, exploration_rate):\n",
    "    '''\n",
    "    Epsilon-Greedy Policy\n",
    "\n",
    "    Generates a random number between 0 and 1 and checks if the current exploration rate (epsilon) is lower.\n",
    "    In case it is lower it returns the index of the highest action value for current state.\n",
    "    Otherwise it chooses exploring and takes a random action\n",
    "    '''\n",
    "    exploration_rate_threshold = random.uniform(0, 1)\n",
    "    if exploration_rate_threshold > exploration_rate:\n",
    "        return greedy(q_table, state)\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms for Rewards and Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_training(rewards_all_episodes, steps_all_episodes, num_episodes):\n",
    "    '''\n",
    "    Evaluates the training by calculating the means and plotting histograms of Rewards and Steps needed during training for 10 different subsets\n",
    "    '''\n",
    "\n",
    "    # Splits both arrays into multiple subarrays\n",
    "    rewards_per_x_episodes = np.split(np.array(rewards_all_episodes),num_episodes/(num_episodes/10))\n",
    "    steps_per_x_episodes = np.split(np.array(steps_all_episodes),num_episodes/(num_episodes/10))\n",
    "    count = num_episodes/10\n",
    "\n",
    "    # Prints average rewards and steps for ten sub intervalls \n",
    "    print(\"********Average reward and steps per \",num_episodes/10 ,\" episodes********\\n\")\n",
    "    for r, s in zip(rewards_per_x_episodes, steps_per_x_episodes):\n",
    "        print(count, \": Reward = \", str(round(sum(r/(num_episodes/10)), 2)), \", Steps = \", str(round(sum(s/(num_episodes/10)), 2)))\n",
    "        count += num_episodes/10\n",
    "\n",
    "    # initialize Reward Histgram Plot\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "    fig.suptitle('Reward Histograms per ' + str(int(num_episodes/10)) + ' episodes during training')\n",
    "\n",
    "    # Plot rewards\n",
    "    for i in range(len(rewards_per_x_episodes)):\n",
    "        count += num_episodes/10\n",
    "        sns.histplot(ax=axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5], x = rewards_per_x_episodes[i])\n",
    "        axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5].set_title( str(len(rewards_per_x_episodes[i])*(i+1)-int(num_episodes/10)) + \" - \" + str(len(rewards_per_x_episodes[i])*(i+1)))\n",
    "\n",
    "    # initialize Step Histgram Plot\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "    fig.suptitle('Step Histograms per ' + str(int(num_episodes/10)) + ' episodes during training')\n",
    "\n",
    "    # Plot Steps\n",
    "    for i in range(len(steps_per_x_episodes)):\n",
    "        count += num_episodes/10\n",
    "        sns.histplot(ax=axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5], x = steps_per_x_episodes[i])\n",
    "        axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5].set_title( str(len(rewards_per_x_episodes[i])*(i+1)-int(num_episodes/10)) + \" - \" + str(len(rewards_per_x_episodes[i])*(i+1)))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineplots for Rewards and Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithm(x,y,plt_type):\n",
    "    '''\n",
    "    Function which compares the different algorithms \n",
    "    '''\n",
    "    x_series = pd.Series(x)\n",
    "    x_series = x_series.rolling(15).mean()\n",
    "    plt.plot(x_series)\n",
    "\n",
    "    y_series = pd.Series(y)\n",
    "    y_series = y_series.rolling(15).mean()\n",
    "    plt.plot(y_series)\n",
    "\n",
    "    plt.legend([\"Q-Learning\", \"SARSA\"])\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(plt_type)\n",
    "\n",
    "    # plt.yscale(\"symlog\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineplots for Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_params(x,y):\n",
    "    '''\n",
    "    Function which compares the same algorithm with different parameters\n",
    "    '''\n",
    "    step_series = pd.Series(x)\n",
    "    step_series = step_series.rolling(15).mean()\n",
    "    plt.plot(step_series)\n",
    "\n",
    "    step_series = pd.Series(y)\n",
    "    step_series = step_series.rolling(15).mean()\n",
    "    #plt.plot(step_series)\n",
    "\n",
    "    #plt.title(\"Sports Watch Data\")\n",
    "    #plt.legend([\"discount_rate = 0.99\", \"discount_rate = 0.2\", \"learning_rate  = 0.8\"])\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "\n",
    "    #plt.yscale(\"symlog\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation on what the agent does after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(env, q_table, max_steps_per_episode, num_vis_episodes=3):\n",
    "    '''\n",
    "    Function which simulates and displays the game a certain amount of times.\n",
    "    '''\n",
    "    # Loop over episodes to visualise\n",
    "    for episode in range(num_vis_episodes):\n",
    "        state, _ = env.reset()\n",
    "        print(\"********EPISODE \", episode+1, \"********\\n\\n\\n\\n\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Render environment for each step and sleep 0.3 seconds\n",
    "        for step in range(max_steps_per_episode):\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            action = np.argmax(q_table[state,:])\n",
    "            new_state, reward, done, truncated, _ = env.step(action)\n",
    "            if done or truncated: \n",
    "                clear_output(wait=True)\n",
    "                print(env.render())\n",
    "                if reward > 0:\n",
    "                    print(\"****You reached the goal!****\")\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"****Something bad happened!****\")\n",
    "                    time.sleep(3)\n",
    "                    clear_output(wait=True)\n",
    "                break\n",
    "            state = new_state\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics for evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(env, num_test_episodes, max_steps_per_episode, q_table):\n",
    "    '''\n",
    "    Function which tests the trained q-table without using exploration\n",
    "    '''\n",
    "    # initialize counters\n",
    "    total_steps, total_penalties, total_reward, total_errors = 0, 0, 0, 0\n",
    "\n",
    "    # Loop over episodes to analyze\n",
    "    for episode in range(num_test_episodes):\n",
    "            state, _ = env.reset()\n",
    "\n",
    "            # Play the game until the agent dies or succeeds and calculate metrics\n",
    "            for step in range(max_steps_per_episode):\n",
    "                total_steps += 1\n",
    "                action = np.argmax(q_table[state,:])\n",
    "                new_state, reward, done, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if reward == -10:\n",
    "                    total_penalties += 1\n",
    "\n",
    "                if done or truncated: \n",
    "                    if reward > 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        total_errors += 1\n",
    "                        break\n",
    "                \n",
    "                if step == max_steps_per_episode  - 1:\n",
    "                    total_errors += 1\n",
    "\n",
    "                state = new_state\n",
    "    \n",
    "    # Print Metrics\n",
    "    print(\"Avg Steps : \", str(round(total_steps/num_test_episodes, 2)))\n",
    "    print(\"Total Penalties: \", str(int(total_penalties)))\n",
    "    print(\"Avg Reward : \", str(round(total_reward/num_test_episodes, 2)))\n",
    "    print(\"Total Errors: \", str(int(total_errors)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Environment and Setup Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDas Ausführen von Zellen mit \"c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\" erfordert das Paket ipykernel.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Joshua/AppData/Local/Programs/Python/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Environment Selection and Hyperparameters\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode='ansi').env # \n",
    "# env = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "# env = gym.make(\"CliffWalking-v0\", render_mode=\"ansi\")\n",
    "\n",
    "params1 = [\n",
    "    0.1,    #learning rate\n",
    "    0.99,   #discount rate\n",
    "    2000,   #num of Episodes\n",
    "    1000,   #max steps per Episode\n",
    "    1,      #exploration rate\n",
    "    1,      #max explaration rate\n",
    "    0.01,   #min exploration rate\n",
    "    0.005,  #exploration decay rate\n",
    "]\n",
    "\n",
    "params2 = [\n",
    "    0.1,    #learning rate\n",
    "    0.99,   #discount rate\n",
    "    1300,   #num of Episodes\n",
    "    1000,   #max steps per Episode\n",
    "    1,      #exploration rate\n",
    "    1,      #max explaration rate\n",
    "    0.01,   #min exploration rate\n",
    "    0.005,  #exploration decay rate\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train different q-Tables with different params and algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q_table, reward_list, step_list \u001b[39m=\u001b[39m train_q_learn(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate)\n\u001b[0;32m      2\u001b[0m \u001b[39m# learning_rate = 0.7\u001b[39;00m\n\u001b[0;32m      3\u001b[0m s_q_table, s_reward_list, s_step_list \u001b[39m=\u001b[39m train_sarsa(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "q_table1, reward_list1, step_list1 = train_q_learn(env, *params1)\n",
    "s_q_table1, s_reward_list1, s_step_list1 = train_sarsa(env, *params1)\n",
    "q_table2, reward_list2, step_list2 = train_q_learn(env, *params2)\n",
    "s_q_table2, s_reward_list2, s_step_list2 = train_sarsa(env, *params2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_algorithm(reward_list1, s_reward_list1,  \"Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_algorithm(step_list1, s_step_list1,  \"Steps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_params(reward_list1, reward_list2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Training with metrics + histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward and steps per  200.0  episodes********\n",
      "\n",
      "200.0 : Reward =  0.02 , Steps =  9.02\n",
      "400.0 : Reward =  0.08 , Steps =  15.18\n",
      "600.0 : Reward =  0.22 , Steps =  23.98\n",
      "800.0 : Reward =  0.34 , Steps =  30.69\n",
      "1000.0 : Reward =  0.48 , Steps =  32.26\n",
      "1200.0 : Reward =  0.45 , Steps =  35.24\n",
      "1400.0 : Reward =  0.5 , Steps =  38.59\n",
      "1600.0 : Reward =  0.52 , Steps =  36.86\n",
      "1800.0 : Reward =  0.53 , Steps =  36.02\n",
      "2000.0 : Reward =  0.56 , Steps =  32.91\n",
      "********Average reward and steps per  200.0  episodes********\n",
      "\n",
      "200.0 : Reward =  0.01 , Steps =  9.17\n",
      "400.0 : Reward =  0.0 , Steps =  12.36\n",
      "600.0 : Reward =  0.04 , Steps =  12.39\n",
      "800.0 : Reward =  0.1 , Steps =  18.89\n",
      "1000.0 : Reward =  0.12 , Steps =  21.91\n",
      "1200.0 : Reward =  0.17 , Steps =  22.12\n",
      "1400.0 : Reward =  0.25 , Steps =  26.39\n",
      "1600.0 : Reward =  0.43 , Steps =  27.54\n",
      "1800.0 : Reward =  0.44 , Steps =  31.59\n",
      "2000.0 : Reward =  0.53 , Steps =  28.33\n"
     ]
    }
   ],
   "source": [
    "eval_training(reward_list1, step_list1, params1[2])\n",
    "eval_training(s_reward_list1, s_step_list1, params1[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise environment and agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise(env, s_q_table1, params1[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics for test scenario without exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Steps :  17.0\n",
      "Total Penalties:  0\n",
      "Avg Reward :  -17.0\n",
      "Total Errors:  100\n"
     ]
    }
   ],
   "source": [
    "eval_test(env, num_test_episodes=100, max_steps_per_episode=params1[3], q_table=s_q_table1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa7e97b85bae90f199f0c94d9acbda0c8fe2596009df7ae1a0455e95b822e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
