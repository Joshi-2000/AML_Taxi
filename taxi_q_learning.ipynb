{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(q_table, state):\n",
    "    '''\n",
    "    Greedy Policy\n",
    "\n",
    "    Returns the index of the highest action value for current state\n",
    "    '''\n",
    "    return np.argmax(q_table[state,:])\n",
    "\n",
    "def eps_greedy(env, q_table, state, exploration_rate):\n",
    "    '''\n",
    "    Epsilon-Greedy Policy\n",
    "\n",
    "    Generates a random number between 0 and 1 and checks if the current exploration rate (epsilon) is lower.\n",
    "    In case it is lower it returns the index of the highest action value for current state.\n",
    "    Otherwise it chooses exploring and takes a random action\n",
    "    '''\n",
    "    exploration_rate_threshold = random.uniform(0, 1)\n",
    "    if exploration_rate_threshold > exploration_rate:\n",
    "        return greedy(q_table, state)\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_q_learn(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate):\n",
    "    '''\n",
    "    Training a q-table with the Q-Learning algorithm\n",
    "    '''\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    # Initialize the q-table with zero values\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # Random generator\n",
    "    rng =np.random.default_rng()\n",
    "\n",
    "    # 2 For life or until learning is stopped\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        rewards_current_episode  = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # 3. Choose an action a in the current world state (s)\n",
    "            action = eps_greedy(env, q_table, state, exploration_rate)\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            # qtable[new_state,:] : all the actions we can take from new state\n",
    "            q_table[state, action] = q_table[state, action] +  learning_rate * (reward + discount_rate * np.max(q_table[new_state, :] - q_table[state, action]))\n",
    "    \n",
    "            \n",
    "            rewards_current_episode  += reward\n",
    "            step += 1\n",
    "            \n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "            \n",
    "            # If done (if we're dead) : finish episode\n",
    "            if done or truncated: \n",
    "                break\n",
    "            \n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(step)\n",
    "    return q_table, rewards_all_episodes, steps_all_episodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_sarsa(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate):\n",
    "    '''\n",
    "    Training a q-table with the SARSA algorithm\n",
    "    '''\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    # Initialize the q-table with zero values\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    print(q_table.shape)\n",
    "\n",
    "    # Random generator\n",
    "    rng =np.random.default_rng()\n",
    "\n",
    "    # 2 For life or until learning is stopped\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        rewards_current_episode  = 0\n",
    "        # Choose an action a in the current world state (s)\n",
    "        action = eps_greedy(env, q_table, state, exploration_rate)\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            \n",
    "            # 3. Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            new_action = eps_greedy(env, q_table, new_state, exploration_rate)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            # qtable[new_state,:] : all the actions we can take from new state\n",
    "            q_table[state, action] = q_table[state, action]  +  learning_rate * (reward + discount_rate * q_table[new_state, new_action] - q_table[state, action])\n",
    "    \n",
    "            \n",
    "            rewards_current_episode  += reward\n",
    "            step += 1\n",
    "            \n",
    "            # Our new state is state\n",
    "            # Our new action is action\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "            \n",
    "            # If done (if we're dead) : finish episode\n",
    "            if done or truncated: \n",
    "                break\n",
    "            \n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(step)\n",
    "        \n",
    "        if episode%50 == 0:\n",
    "            print(exploration_rate)\n",
    "\n",
    "        \n",
    "    return q_table, rewards_all_episodes, steps_all_episodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_training(rewards_all_episodes, steps_all_episodes, num_episodes):\n",
    "    '''\n",
    "    Evaluates the training by calculating the means and plotting histograms of Rewards and Steps needed during training for 10 different subsets\n",
    "    '''\n",
    "    rewards_per_x_episodes = np.split(np.array(rewards_all_episodes),num_episodes/(num_episodes/10))\n",
    "    steps_per_x_episodes = np.split(np.array(steps_all_episodes),num_episodes/(num_episodes/10))\n",
    "    count = num_episodes/10\n",
    "\n",
    "    print(\"********Average reward and steps per \",num_episodes/10 ,\" episodes********\\n\")\n",
    "    for r, s in zip(rewards_per_x_episodes, steps_per_x_episodes):\n",
    "        print(count, \": Reward = \", str(round(sum(r/(num_episodes/10)), 2)), \", Steps = \", str(round(sum(s/(num_episodes/10)), 2)))\n",
    "        count += num_episodes/10\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "    fig.suptitle('Reward Histograms per ' + str(int(num_episodes/10)) + ' episodes during training')\n",
    "\n",
    "    for i in range(len(rewards_per_x_episodes)):\n",
    "        count += num_episodes/10\n",
    "        sns.histplot(ax=axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5], x = rewards_per_x_episodes[i])\n",
    "        axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5].set_title( str(len(rewards_per_x_episodes[i])*(i+1)-int(num_episodes/10)) + \" - \" + str(len(rewards_per_x_episodes[i])*(i+1)))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "    fig.suptitle('Step Histograms per ' + str(int(num_episodes/10)) + ' episodes during training')\n",
    "\n",
    "    for i in range(len(steps_per_x_episodes)):\n",
    "        count += num_episodes/10\n",
    "        sns.histplot(ax=axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5], x = steps_per_x_episodes[i])\n",
    "        axes[int(5%(i+1)/5), i-int(5%(i+1)/5)*5].set_title( str(len(rewards_per_x_episodes[i])*(i+1)-int(num_episodes/10)) + \" - \" + str(len(rewards_per_x_episodes[i])*(i+1)))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(rewards_all_episodes,s_rewards_all_episodes, steps_all_episodes, s_steps_all_episodes, num_episodes):\n",
    "\n",
    "    df_rewards = pd.DataFrame(list(zip(rewards_all_episodes, s_rewards_all_episodes)), \n",
    "                                              columns =['Q_Rewards', 'S_Rewards'])\n",
    "    \n",
    "    df_steps = pd.DataFrame(list(zip(steps_all_episodes, s_steps_all_episodes)), \n",
    "                                              columns =['Q_Steps', 'S_Steps'])\n",
    "    \n",
    "    df_moving_rewards = df_rewards.rolling(10).mean().shift(-4)\n",
    "    df_moving_steps = df_steps.rolling(10).mean().shift(-4)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(25, 10))\n",
    "    g = sns.lineplot(data = df_rewards,  ax = axes[0,0])\n",
    "    g.set(yscale='symlog')\n",
    "    g.lines[1].set_linestyle(\"solid\")\n",
    "    g.set_xlabel('episodes')\n",
    "    leg = g.legend()\n",
    "    leg_lines = leg.get_lines()\n",
    "    leg_lines[1].set_linestyle(\"solid\")\n",
    "    g = sns.lineplot(data = df_steps, ax = axes[0,1])\n",
    "    g.set(yscale=\"log\")\n",
    "    g.lines[1].set_linestyle(\"solid\")\n",
    "    g.set_xlabel('episodes')\n",
    "    leg = g.legend()\n",
    "    leg_lines = leg.get_lines()\n",
    "    leg_lines[1].set_linestyle(\"solid\")\n",
    "    g = sns.lineplot(data = df_moving_rewards, ax = axes[1,0])\n",
    "    g.set(yscale='symlog')\n",
    "    g.lines[1].set_linestyle(\"solid\")\n",
    "    g.set_xlabel('episodes')\n",
    "    leg = g.legend()\n",
    "    leg_lines = leg.get_lines()\n",
    "    leg_lines[1].set_linestyle(\"solid\")\n",
    "    g = sns.lineplot(data = df_moving_steps, ax = axes[1,1])\n",
    "    g.set(yscale=\"log\")\n",
    "    g.lines[1].set_linestyle(\"solid\")\n",
    "    g.set_xlabel('episodes')\n",
    "    leg = g.legend()\n",
    "    leg_lines = leg.get_lines()\n",
    "    leg_lines[1].set_linestyle(\"solid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(x,y,plt_type):\n",
    "    x_series = pd.Series(x)\n",
    "    x_series = x_series.rolling(15).mean()\n",
    "    plt.plot(x_series)\n",
    "\n",
    "    y_series = pd.Series(y)\n",
    "    y_series = y_series.rolling(15).mean()\n",
    "    plt.plot(y_series)\n",
    "\n",
    "    plt.legend([\"Q-Learning\", \"SARSA\"])\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(plt_type)\n",
    "\n",
    "    # plt.yscale(\"symlog\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(env, q_table, max_steps_per_episode, num_vis_episodes=3):\n",
    "    '''\n",
    "    Function which simulates and displays the game a certain amount of times.\n",
    "    '''\n",
    "    for episode in range(num_vis_episodes):\n",
    "        state, _ = env.reset()\n",
    "        print(\"********EPISODE \", episode+1, \"********\\n\\n\\n\\n\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            action = np.argmax(q_table[state,:])\n",
    "            new_state, reward, done, truncated, _ = env.step(action)\n",
    "            if done or truncated: \n",
    "                clear_output(wait=True)\n",
    "                print(env.render())\n",
    "                if reward > 0:\n",
    "                    print(\"****You reached the goal!****\")\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"****Something else happened!****\")\n",
    "                    time.sleep(3)\n",
    "                    clear_output(wait=True)\n",
    "                break\n",
    "            state = new_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(env, num_test_episodes, max_steps_per_episode, q_table):\n",
    "    total_steps, total_penalties, total_reward, total_errors = 0, 0, 0, 0\n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "            state, _ = env.reset()\n",
    "\n",
    "            for step in range(max_steps_per_episode):\n",
    "                total_steps += 1\n",
    "                action = np.argmax(q_table[state,:])\n",
    "                new_state, reward, done, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if reward == -10:\n",
    "                    total_penalties += 1\n",
    "\n",
    "                if done or truncated: \n",
    "                    if reward > 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        total_errors += 1\n",
    "                        break\n",
    "                \n",
    "                if step == max_steps_per_episode  - 1:\n",
    "                    total_errors += 1\n",
    "\n",
    "                state = new_state\n",
    "    \n",
    "    print(\"Avg Steps : \", str(round(total_steps/num_test_episodes, 2)))\n",
    "    print(\"Total Penalties: \", str(int(total_penalties)))\n",
    "    print(\"Avg Reward : \", str(round(total_reward/num_test_episodes, 2)))\n",
    "    print(\"Total Errors: \", str(int(total_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode='ansi').env # \n",
    "# env = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "# env = gym.make(\"CliffWalking-v0\", render_mode=\"ansi\")\n",
    "\n",
    "num_episodes = 2000\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1 # epsilon\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.05\n",
    "exploration_decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n",
      "1.0\n",
      "0.7898607439178347\n",
      "0.6262041267270018\n",
      "0.49874822510396394\n",
      "0.3994854691128702\n",
      "0.32217955701718054\n",
      "0.2619736521410083\n",
      "0.21508524627792286\n",
      "0.17856851907478205\n",
      "0.15012926333377113\n",
      "0.12798074869270387\n",
      "0.11073146814637219\n",
      "0.09729771494947075\n",
      "0.08683549744013591\n",
      "0.07868751425120257\n",
      "0.07234185856320866\n",
      "0.06739985694429747\n",
      "0.0635510222135493\n",
      "0.06055354671133019\n",
      "0.05821911044296461\n",
      "0.056401049649131195\n",
      "0.054985142479222315\n",
      "0.05388243286654087\n",
      "0.053023641756684184\n",
      "0.052354814567833044\n",
      "0.05183393142941633\n",
      "0.051428267233328696\n",
      "0.05111233563975162\n",
      "0.050866287867276795\n",
      "0.050674665669400425\n",
      "0.050525430151640446\n",
      "0.050409205413546904\n",
      "0.05031868949650739\n",
      "0.05024819562943659\n",
      "0.050193294950560116\n",
      "0.05015053825885997\n",
      "0.05011723931388235\n",
      "0.05009130606945833\n",
      "0.05007110923839332\n",
      "0.050055379930544326\n"
     ]
    }
   ],
   "source": [
    "q_table, reward_list, step_list = train_q_learn(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate)\n",
    "# learning_rate = 0.7\n",
    "s_q_table, s_reward_list, s_step_list = train_sarsa(env, learning_rate, discount_rate, num_episodes, max_steps_per_episode, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BaneBoy\\Documents\\Projekte\\AML_Taxi\\env\\lib\\site-packages\\ipykernel\\eventloops.py:128: UserWarning: All values for SymLogScale are below linthresh, making it effectively linear. You likely should lower the value of linthresh. \n",
      "  el.exec() if hasattr(el, 'exec') else el.exec_()\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "compare(reward_list, s_reward_list,  step_list, s_step_list, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "compare_plot(reward_list, s_reward_list,  \"Rewards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot(step_list, s_step_list,  \"Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward and steps per  200.0  episodes********\n",
      "\n",
      "200.0 : Reward =  0.02 , Steps =  9.02\n",
      "400.0 : Reward =  0.08 , Steps =  15.18\n",
      "600.0 : Reward =  0.22 , Steps =  23.98\n",
      "800.0 : Reward =  0.34 , Steps =  30.69\n",
      "1000.0 : Reward =  0.48 , Steps =  32.26\n",
      "1200.0 : Reward =  0.45 , Steps =  35.24\n",
      "1400.0 : Reward =  0.5 , Steps =  38.59\n",
      "1600.0 : Reward =  0.52 , Steps =  36.86\n",
      "1800.0 : Reward =  0.53 , Steps =  36.02\n",
      "2000.0 : Reward =  0.56 , Steps =  32.91\n",
      "********Average reward and steps per  200.0  episodes********\n",
      "\n",
      "200.0 : Reward =  0.01 , Steps =  9.17\n",
      "400.0 : Reward =  0.0 , Steps =  12.36\n",
      "600.0 : Reward =  0.04 , Steps =  12.39\n",
      "800.0 : Reward =  0.1 , Steps =  18.89\n",
      "1000.0 : Reward =  0.12 , Steps =  21.91\n",
      "1200.0 : Reward =  0.17 , Steps =  22.12\n",
      "1400.0 : Reward =  0.25 , Steps =  26.39\n",
      "1600.0 : Reward =  0.43 , Steps =  27.54\n",
      "1800.0 : Reward =  0.44 , Steps =  31.59\n",
      "2000.0 : Reward =  0.53 , Steps =  28.33\n"
     ]
    }
   ],
   "source": [
    "eval_training(reward_list, step_list, num_episodes)\n",
    "eval_training(s_reward_list, s_step_list, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "****You reached the goal!****\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m visualise(env, s_q_table, max_steps_per_episode)\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mvisualise\u001b[1;34m(env, q_table, max_steps_per_episode, num_vis_episodes)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m reward \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m****You reached the goal!****\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m****Something else happened!****\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "visualise(env, s_q_table, max_steps_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Steps :  17.0\n",
      "Total Penalties:  0\n",
      "Avg Reward :  -17.0\n",
      "Total Errors:  100\n"
     ]
    }
   ],
   "source": [
    "eval_test(env, num_test_episodes=100, max_steps_per_episode=max_steps_per_episode, q_table=s_q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(observation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'observation' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa7e97b85bae90f199f0c94d9acbda0c8fe2596009df7ae1a0455e95b822e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
